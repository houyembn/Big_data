# input {
#   # Input générique pour les fichiers CSV et JSON
#   file {
#     path => "/usr/share/logstash/logs/*.{csv,json}"  # Accepte les fichiers CSV et JSON
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     discover_interval => 10  # Vérifie les nouveaux fichiers toutes les 10 secondes
#   }
# }

# filter {
#   if [path] =~ "\.csv$" {
#     csv {
#       separator => ","
#       autodetect_column_names => true  # Détection automatique des noms de colonnes
#       skip_header => true  # Ignore la première ligne si c'est une en-tête
#     }
#   }

#   if [path] =~ "\.json$" {
  
#   }
# }

# # output {
# #   stdout { codec => rubydebug }  # Affiche les données dans la console pour le débogage
# #   elasticsearch {
# #     hosts => ["http://elasticsearch:9200"]  # Nom du conteneur Elasticsearch
# #     index => "logstash-%{+YYYY.MM.dd}"  # Crée un index quotidien
# #     document_type => "_doc"  # Utilisation du type par défaut "_doc"
# #   }
# output {
#   stdout { codec => rubydebug }

#   if [path] =~ "\.csv$" {
#     elasticsearch {
#       hosts => ["http://elasticsearch:9200"]
#       index => "logstash-csv-%{+YYYY.MM.dd}"
#       document_type => "_doc"
#     }
#   }

#   if [path] =~ "\.json$" {
#     elasticsearch {
#       hosts => ["http://elasticsearch:9200"]
#       index => "logstash-json-%{+YYYY.MM.dd}"
#       document_type => "_doc"
#     }
#   }


# }

# CSV Input
# input {
#   file {
#     path => "/usr/share/logstash/logs/*.csv"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     discover_interval => 10
#   }
# }

# # CSV Filter
# filter {
#   if [path] =~ /\.csv$/ {
#     csv {
#       separator => ","
#       autodetect_column_names => true
#       skip_header => true
#       columns => ["LineId", "Label", "Timestamp", "Date", "Node", "Time", "NodeRepeat", "Type"," Component","Level", "Content","EvenId","EventTemplate"]
#     }
#   }
# }

# # CSV Output
# output {
#   if [path] =~ /\.csv$/ {
#     stdout { codec => rubydebug }
#     elasticsearch {
#       hosts => ["http://elasticsearch:9200"]
#       index => "logstash-csvu-%{+YYYY.MM.dd}"
#     }
#   }
# }

# # JSON Input
# input {
#   file {
#     path => "/usr/share/logstash/logs/*.json"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     discover_interval => 10
#   }
# }

# # JSON Filter
# filter {
#  if [path] =~ /\.json$/ {
#     json {
#       source => "message"
#     }
#   }
# }

# # JSON Output
# output {
#   if [path] =~ /\.json$/ {
#     stdout { codec => rubydebug }
#     elasticsearch {
#       hosts => ["http://elasticsearch:9200"]
#       index => "logstash-jsonu-%{+YYYY.MM.dd}"
#     }
#   }
# }

# # LOG Input
# input {
#   file {
#     path => "/usr/share/logstash/logs/*.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     codec => plain { charset => "UTF-8" }
#   }
# }

# # LOG Filter
# filter {
#    if [path] =~ /\.log$/ {
#     grok {
#       match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:loglevel} %{GREEDYDATA:logmessage}" }
#     }
#     date {
#       match => ["timestamp", "ISO8601"]
#       target => "@timestamp"
#     }
#   }
# }

# # LOG Output
# output {
#   if [path] =~ /\.log$/ {
#     stdout { codec => rubydebug }
#     elasticsearch {
#       hosts => ["http://elasticsearch:9200"]
#       index => "logstash-logsu-%{+YYYY.MM.dd}"
#     }
#   }
# }


input {
  # lire fichier csv
    file {
		path => "/usr/share/logstash/logs/*.csv"
		start_position => "beginning"
		sincedb_path => "/dev/null"
		mode => "tail"
		codec => plain { charset => "UTF-8" } # Pas besoin de multiline
		type => "csv" 
		discover_interval => 15 
    }
  
  # lire fichier (JSON)
    file {
        path => "/usr/share/logstash/logs/*.json"
        type => "json"
        start_position => "beginning"
        sincedb_path => "/dev/null"
        codec => json  # Utiliser json_lines pour des objets JSON séparés par des nouvelles lignes
        type => "json"
        discover_interval => 15
    }
}

filter {
  # Simplify filters for initial testing
  
    if [type] == "csv" {  # Vérifier si le type de l'entrée est CSV
        csv {
            separator => ","  # Définir le séparateur (virgule par défaut pour CSV)
            autodetect_column_names => true  # Détection automatique des noms de colonnes
            skip_header => true # Ignorer la première ligne contenant les noms des colonnes
            # columns => ["LineId", "Date", "Time", "Level", "Process", "Component", "Content", "EventId", "EventTemplate", "Pid", "User" , "Timestamp"]
        }
        
        date {
            match => ["Date", "yyyy-MM-dd"]
            target => "timestamp"  # Cela place la date au bon endroit
            remove_field => ["Date"]  # Vous pouvez enlever le champ Date s'il n'est plus nécessaire
        }

        mutate {
          gsub => [
            "EventTemplate", "<\\*>", ""  # Remplacer les occurrences de <*>
          ]
        }
    }
    else if [type] == "json" {
       mutate {
          gsub => [
            "message", "},\s*{", "}|{"  # Ajoute un séparateur entre objets JSON
          ]
        }
        json {
          source => "message"  # Assurez-vous que "message" est le champ contenant le JSON
        }
       
        
    } 
}

output {
    stdout { codec => rubydebug }    # Affiche les données dans la console pour le débogage
    elasticsearch {
        hosts => ["http://elasticsearch:9200"]
        index => "%{type}8-%{+YYYY.MM.dd}"
        action => "index"
        document_type => "_doc" 

    }
    
   
}